{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment analysis of the Persian text with word embedding & frequency word .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Szivt7qr79nm",
        "N-GSfYOVXF8t",
        "bnOyG5AcbAir",
        "HLpJAUUC7SFz",
        "HIa-bRVB7_cp",
        "Mhounpfs8GRZ",
        "E1MZtoBo8I5o",
        "GT2_TkQu7s7c",
        "6fbj-Mx47vdB",
        "eR7Q0S237y-h",
        "o-mj92wt72gS"
      ],
      "authorship_tag": "ABX9TyNCR2iRydEb4zA1HphYUlnZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanazzzmi/Sentiment_analysis_of_the_Persian_text_with_word_embedding_-_frequency_word/blob/main/Sentiment_analysis_of_the_Persian_text_with_word_embedding_%26_frequency_word_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### install and import data"
      ],
      "metadata": {
        "id": "Szivt7qr79nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[download lexicon url]( https://github.com/Text-Mining/Persian-Sentiment-Resources/blob/master/PerSent.xlsx)"
      ],
      "metadata": {
        "id": "nmA-rhDP8ivP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1ol0fYR9-X2",
        "outputId": "30de7337-fce4-4db8-8d6a-f106b6409588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm\n",
        "!test -f resources-0.5.zip || curl -LO https://github.com/sobhe/hazm/releases/download/v0.5/resources-0.5.zip\n",
        "!test -d resources || ( mkdir -p resources && cd resources && unzip ../resources-0.5.zip )\n",
        "\n",
        "import hazm\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "#dataset\n",
        "!wget https://raw.githubusercontent.com/ashalogic/Persian-Sentiment-Analyzer/master/Tutorial_Dataset.csv\n",
        "csv_dataset = pd.read_csv(\"/content/Tutorial_Dataset.csv\")\n",
        "\n",
        "#lexicon\n",
        "################## Saved address of the downloaded lexicon file #####################\n",
        "\n",
        "lexicon = pd.read_excel(\"/content/drive/MyDrive/PerSent.xlsx\")\n",
        "\n",
        "################### pd.read_excel(\"Saved address of the downloaded lexicon file\") ###"
      ],
      "metadata": {
        "id": "atVCnVmY4RQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "477f4c8b-ec69-48f9-9bcc-da1a065c95fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 10.3 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 37.3 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394488 sha256=96b32b6504547810482b24cc430e12793b3c6ddcc2a2677f2f46a01f1050f544\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=153943 sha256=26bebc6d02b3bccf109682a17a5a1357f78ee6c31e2d9bde0aab685f2e3538f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   654    0   654    0     0   1424      0 --:--:-- --:--:-- --:--:--  1421\n",
            "100 29.1M  100 29.1M    0     0  37.0M      0 --:--:-- --:--:-- --:--:-- 37.0M\n",
            "Archive:  ../resources-0.5.zip\n",
            "  inflating: chunker.model           \n",
            "  inflating: langModel.mco           \n",
            "   creating: lib/\n",
            "  inflating: lib/liblinear-1.8.jar   \n",
            "  inflating: lib/libsvm.jar          \n",
            "  inflating: lib/log4j.jar           \n",
            "  inflating: malt.jar                \n",
            "  inflating: postagger.model         \n",
            "--2022-01-25 09:54:13--  https://raw.githubusercontent.com/ashalogic/Persian-Sentiment-Analyzer/master/Tutorial_Dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1444954 (1.4M) [text/plain]\n",
            "Saving to: ‘Tutorial_Dataset.csv’\n",
            "\n",
            "Tutorial_Dataset.cs 100%[===================>]   1.38M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2022-01-25 09:54:13 (150 MB/s) - ‘Tutorial_Dataset.csv’ saved [1444954/1444954]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
        "!gunzip /content/cc.fa.300.bin.gz\n",
        "!pip install fasttext\n",
        "\n",
        "import fasttext \n",
        "\n",
        "%time\n",
        "model = fasttext.load_model(\"/content/cc.fa.300.bin\")"
      ],
      "metadata": {
        "id": "iQeNWHff4RUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4406e06-b82a-4cbb-c59b-48114ad9053d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-25 09:54:14--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4502524724 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.fa.300.bin.gz’\n",
            "\n",
            "cc.fa.300.bin.gz    100%[===================>]   4.19G  33.9MB/s    in 1m 55s  \n",
            "\n",
            "2022-01-25 09:56:10 (37.5 MB/s) - ‘cc.fa.300.bin.gz’ saved [4502524724/4502524724]\n",
            "\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.0-py2.py3-none-any.whl (210 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3127585 sha256=7f24c2ba7fb8e774c0127430faf4d6d6e71bb9cd0b7a7acb425e8a027aaf973c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.0\n",
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 6.68 µs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-GSfYOVXF8t"
      },
      "source": [
        "### train test split for CNN modle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4WLV4-mbfe6",
        "outputId": "495e6292-5e35-4bfd-b8db-7ab5a0d16d52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posetive count 2382\n",
            "Negetive count 460\n",
            "Natural  count 419\n",
            "\n",
            "Total    count 3261\n",
            "\n",
            "Posetive count :  \n",
            " ['لطفا هر چه زود\\u200cتر موجود بشه میخام بخرم خیلی ممنون ', 1]\n",
            "Negetive count :  \n",
            " ['هزینه خرید کارتریج بعد از مدت کوتاهی از هزینه خود دستگاه بیشتر میشه کارتیج این دستگاه خیلی گرونه ', 3]\n",
            "unknown  count :  \n",
            " ['من ۱۰ ماه هست گرفتم و ازش راضی بودم تا اینکه امروز در بین کار خاموش شد و دیگه روشن نشد ', 2]\n",
            "Total    count 900\n"
          ]
        }
      ],
      "source": [
        "def CleanPersianText(text):\n",
        "  _normalizer = hazm.Normalizer()\n",
        "  text = _normalizer.normalize(text)\n",
        "  return text\n",
        "\n",
        "revlist = list(map(lambda x: [CleanPersianText(x[0]),x[1]],zip(csv_dataset['Text'],csv_dataset['Suggestion'])))\n",
        "pos=list(filter(lambda x: x[1] == 1,revlist))\n",
        "nat=list(filter(lambda x: x[1] == 2,revlist))\n",
        "neg=list(filter(lambda x: x[1] == 3,revlist))\n",
        "\n",
        "revlist_shuffle = pos[:450] + neg[:450]\n",
        "random.shuffle(revlist_shuffle)\n",
        "\n",
        "print(\"Posetive count {}\".format(len(pos)))\n",
        "print(\"Negetive count {}\".format(len(neg)))\n",
        "print(\"Natural  count {}\".format(len(nat)))\n",
        "print()\n",
        "print(\"Total    count {}\".format(len(revlist)))\n",
        "print()\n",
        "print(\"Posetive count : \",\"\\n\",pos[random.randrange(1,len(pos))])\n",
        "print(\"Negetive count : \",\"\\n\",neg[random.randrange(1,len(neg))])\n",
        "print(\"unknown  count : \",\"\\n\",nat[random.randrange(1,len(nat))])\n",
        "print(\"Total    count {}\".format(len(revlist_shuffle)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gEYJuyHdpIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "869acf10-b700-4eab-e509-5cc7b0adcd53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((810, 20, 300), (90, 20, 300), (810, 2), (90, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "vector_size = 300 \n",
        "max_no_tokens = 20 \n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "train_size = int(0.9*(len(revlist_shuffle)))\n",
        "test_size = int(0.1*(len(revlist_shuffle)))#lstm\n",
        "indexes = set(np.random.choice(len(revlist_shuffle), train_size + test_size, replace=False))\n",
        "\n",
        "x_train = np.zeros((train_size, max_no_tokens, vector_size), dtype=K.floatx())\n",
        "y_train = np.zeros((train_size, 2), dtype=np.int32)\n",
        "\n",
        "x_test = np.zeros((test_size, max_no_tokens, vector_size), dtype=K.floatx())\n",
        "y_test = np.zeros((test_size, 2), dtype=np.int32)\n",
        "\n",
        "\n",
        "for i, index in enumerate(indexes):\n",
        "  text_words = hazm.word_tokenize(revlist_shuffle[index][0])\n",
        "  for t in range(0,len(text_words)):\n",
        "    if t >= max_no_tokens:\n",
        "      break\n",
        "\n",
        "    if text_words[t] not in model.words:\n",
        "      continue\n",
        "    if i < train_size:\n",
        "      x_train[i, t, :] = model.get_word_vector(text_words[t])\n",
        "    else:\n",
        "      x_test[i - train_size, t, :] = model.get_word_vector(text_words[t])\n",
        "\n",
        "  if i < train_size:\n",
        "    y_train[i, :] = [1.0, 0.0] if revlist_shuffle[index][1] == 3 else [0.0, 1.0]\n",
        "  else:\n",
        "    y_test[i - train_size, :] = [1.0, 0.0] if revlist_shuffle[index][1] == 3 else [0.0, 1.0]\n",
        "   \n",
        "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V2xniiAO5BCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FQA6ms841GL"
      },
      "source": [
        "### **Article idea**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI2micWT93Zg"
      },
      "outputs": [],
      "source": [
        "text = [x[0] for x in revlist_shuffle ]\n",
        "y = [x[1] for x in revlist_shuffle ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(text)"
      ],
      "metadata": {
        "id": "S9UdeCTycgqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size  = len(token.word_index) + 1\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "0nJgJCnScixg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7MG7gv3USkk",
        "outputId": "d7b91504-16ea-4123-a5d0-96c3ca203650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[171, 14, 1426, 122, 70, 5, 159, 2, 30, 101, 26, 47, 61], [36, 17, 42, 93, 11, 3154, 1, 930, 15], [4, 14, 18, 27, 61, 413, 34, 25, 4, 79, 2160, 2, 106, 23, 28, 71, 164, 310, 8, 437, 7, 43, 103, 1, 104, 611, 94, 845, 1234, 11, 2161]]\n"
          ]
        }
      ],
      "source": [
        "encoded_text = token.texts_to_sequences(text)\n",
        "print(encoded_text[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srPMESD4USrM"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_length = 120\n",
        "X = pad_sequences(encoded_text, maxlen=max_length, padding='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJV0ntWtUXXk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "d = 300\n",
        "word_vector_matrix = np.zeros((vocab_size, d))\n",
        "\n",
        "\n",
        "not_word = []\n",
        "have_word = []\n",
        "\n",
        "for word, index in token.word_index.items()  :# هر کلمه یه شناسه داره\n",
        "    vector = model.get_word_vector(word)\n",
        "    if vector is not None:\n",
        "        word_vector_matrix[index] = vector\n",
        "        have_word.append(word)\n",
        "    else:\n",
        "        not_word.append(word)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShkDsSQLZARB"
      },
      "outputs": [],
      "source": [
        "yy = []\n",
        "for x in y:\n",
        "  if x == 1:\n",
        "    yy.append(1)\n",
        "  else:\n",
        "    yy.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTjoVar4UXaN"
      },
      "outputs": [],
      "source": [
        "#Divide texts by labels one and zero\n",
        "df_y = pd.DataFrame(np.array(yy),columns=['a'])\n",
        "df_one = df_y[df_y['a']==1].index.values.astype(int)\n",
        "df_zero = df_y[df_y['a']==0].index.values.astype(int)\n",
        "\n",
        "text_one = []\n",
        "for x in df_one:\n",
        "    text_one.append(text[x])\n",
        "\n",
        "text_zero = []\n",
        "for x in df_zero:\n",
        "    text_zero.append(text[x])\n",
        "\n",
        "join_text_zero = [' '.join(text_zero)]\n",
        "join_text_one = [' '.join(text_one)]\n",
        "\n",
        "df_join_text_zero = pd.DataFrame(join_text_zero[0:],columns= [\"tex\"])\n",
        "df_join_text_one = pd.DataFrame(join_text_one[0:],columns= [\"tex\"])\n",
        "\n",
        "df_have_word = pd.DataFrame({'word':have_word})\n",
        "df_token = pd.DataFrame.from_dict(token.word_index.items())\n",
        "\n",
        "df_text_one = pd.DataFrame(text_one[0:],columns= [\"tex\"])\n",
        "df_text_zero = pd.DataFrame(text_zero[0:],columns= [\"tex\"])\n",
        "\n",
        "from collections import Counter\n",
        "count_zero = df_join_text_zero['tex'].str.split().apply(Counter)\n",
        "count_one = df_join_text_one['tex'].str.split().apply(Counter)\n",
        "\n",
        "df_counter_zero = pd.DataFrame.from_dict(count_zero[0], orient='index').reset_index()\n",
        "df_counter_one = pd.DataFrame.from_dict(count_one[0], orient='index').reset_index()\n",
        "df_counter_zero.columns = ['word', 'value']\n",
        "df_counter_one.columns = ['word', 'value']\n",
        "\n",
        "df_token.columns = ['word', 'id']\n",
        "\n",
        "sentiment_zero =df_have_word.merge(df_counter_zero, left_on='word', right_on='word')[['value','word']]\n",
        "sentiment_one = df_have_word.merge(df_counter_one, left_on='word', right_on='word')[['value','word']]\n",
        "\n",
        "#Words found in both positive and negative sentences\n",
        "common = sentiment_one.merge(sentiment_zero,on=['word','word'])\n",
        "common.columns = ['value_one', 'word' , 'value_zero']\n",
        "\n",
        "common[\"max\"] = common[[\"value_one\", \"value_zero\"]].max(axis=1)\n",
        "common[\"min\"] = common[[\"value_one\", \"value_zero\"]].min(axis=1)\n",
        "\n",
        "\n",
        "common[\"lable\"] = np.where(common[\"value_one\"] == common[\"max\"], 1, 0)\n",
        "common[\"frequent_negative_sentence\"] = np.where(common[\"lable\"] == 1 , common[\"min\"], common[\"max\"])\n",
        "common[\"frequent_positive_sentence\"] = np.where(common[\"lable\"] == 0 , common[\"min\"], common[\"max\"])\n",
        "\n",
        "common_df_final = common.drop('value_one', 1)\n",
        "common_df_final = common_df_final.drop('value_zero', 1)\n",
        "common_df_final = common_df_final.drop('max', 1)\n",
        "common_df_final = common_df_final.drop('min', 1)\n",
        "\n",
        "#Those words that are seen in positive sentences and not seen in negative sentences\n",
        "just_one = sentiment_one[(~sentiment_one.word.isin(common.word))&(~sentiment_one.word.isin(common.word))]\n",
        "just_one = just_one.assign(lable = 1)\n",
        "just_one = just_one.assign(frequent_negative_sentence = 0)\n",
        "just_one.columns = ['frequent_positive_sentence', 'word' , 'lable','frequent_negative_sentence']\n",
        "\n",
        "#Those words that are seen in negative sentences and not seen in positive sentences\n",
        "just_zero = sentiment_zero[(~sentiment_zero.word.isin(common.word))&(~sentiment_zero.word.isin(common.word))]\n",
        "just_zero = just_zero.assign(lable = 0)\n",
        "just_zero = just_zero.assign(frequent_posoitve_sentence = 0)\n",
        "just_zero.columns = ['frequent_negative_sentence', 'word' , 'lable','frequent_positive_sentence']\n",
        "\n",
        "frames = [common_df_final,just_zero, just_one]\n",
        "result = pd.concat(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQc1HykjUXcb"
      },
      "outputs": [],
      "source": [
        "result['WI_positive_word'] = np.where(result['lable']== 1 , 1- result['frequent_negative_sentence']/ result['frequent_positive_sentence'], None)\n",
        "result['WI_negative_word'] = np.where(result['lable']== 0 , 1- result['frequent_positive_sentence']/ result['frequent_negative_sentence'], None)\n",
        "result['WI'] = result['WI_positive_word'].fillna(result['WI_negative_word'])\n",
        "\n",
        "#result.index[result['lable'] == 0].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NgzkiApc9Qh"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(list(zip(text, y)), columns =['text', 'lable'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2JThyUvcbA8"
      },
      "outputs": [],
      "source": [
        "word_have_list= list(result.word)\n",
        "word_have_set = set(word_have_list)\n",
        "result_word_list = result['word'].tolist()\n",
        "result_WI_list = result['WI'].tolist()\n",
        "\n",
        "def word_WI_finder(x):\n",
        "    df_words = set(x.split(' '))\n",
        "    extract_words =  word_set.intersection(df_words)\n",
        "    #index_word = result_word_list.index(extract_words)\n",
        "    \n",
        "    \n",
        "    return (extract_words)\n",
        "\n",
        "\n",
        "word_set = word_have_set\n",
        "\n",
        "df['ddd_WI'] = df.text.apply(word_WI_finder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYjyFARcczP7"
      },
      "outputs": [],
      "source": [
        "WI_ww = []\n",
        "for x in df['ddd_WI'][:]:\n",
        "    WI_w = []\n",
        "    for y in x : \n",
        "        ind = result_word_list.index(y)\n",
        "        WI_w.append(result_WI_list[ind])\n",
        "    WI_ww.append(WI_w)\n",
        "    \n",
        "WI_ww_new = [[] if x==[0] else x for x in WI_ww]\n",
        "WI_ww_neww = [[] if x==[0,0] else x for x in WI_ww_new]\n",
        "Weight_sentence = [[float(j)/sum(i) for j in i] for i in WI_ww_neww[:]]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NQbXmvEdjAJ"
      },
      "outputs": [],
      "source": [
        "sentence_ss = []\n",
        "for i in range(len(df['ddd_WI'][:])):\n",
        "    #print(df['ddd_WI'][x])\n",
        "    vv = []\n",
        "    for j in range(len(df['ddd_WI'][i])):\n",
        "        #print(list(df['ddd_WI'][i])[j])\n",
        "        if len(Weight_sentence[i]) > 1:\n",
        "            v = []\n",
        "            for num in range(d):\n",
        "                vec = Weight_sentence[i][j] * float(model.get_word_vector(list(df['ddd_WI'][i])[j])[num])\n",
        "                v.append(vec)\n",
        "            #print(v)\n",
        "            vv.append(v)\n",
        "        #print(vv)\n",
        "        else:\n",
        "            vv.append(model.get_word_vector(list(df['ddd_WI'][i])[j]).astype(np.float))\n",
        "            \n",
        "    s = [sum(x) for x in zip(*vv)]\n",
        "    sentence_ss.append(s)\n",
        "\n",
        "sentence_ss[0]\n",
        "\n",
        "data = pd.DataFrame(sentence_ss[0:] , columns = list(range(d)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hseZiUZhBJh"
      },
      "outputs": [],
      "source": [
        "r = [a for a in range(len(data))]\n",
        "random.shuffle(r)\n",
        "train_size = int(0.9*(len(r)))\n",
        "test_size = int(0.1*(len(r)))\n",
        "train_index = r[:train_size]\n",
        "test_index = r[train_size:]\n",
        "x_train = data.iloc[train_index]\n",
        "x_test = data.iloc[test_index]\n",
        "\n",
        "li_y_train = [yy[i] for i in train_index]\n",
        "li_y_test = [yy[i] for i in test_index]\n",
        "y_train = pd.DataFrame(li_y_train)\n",
        "y_test = pd.DataFrame(li_y_test)\n",
        "y_train = pd.DataFrame(li_y_train)\n",
        "y_test = pd.DataFrame(li_y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnOyG5AcbAir"
      },
      "source": [
        "### xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEMmFgrDjOQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e040bb-a144-45e8-b525-864413a1a0c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45.55555555555556\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "dtrain = xgb.DMatrix(x_train , label = y_train)\n",
        "dtest = xgb.DMatrix(x_test , label = y_test)\n",
        "param = {\n",
        "    'max_depth': 10,  # the maximum depth of each tree\n",
        "    'eta': 0.1,  # the training step for each iteration\n",
        "    #'silent': 1,  # logging mode - quiet\n",
        "    'objective': 'binary:logistic',  \n",
        "    'eval_metric': 'error',\n",
        "    #'num_class': 2}  # the number of classes that exist in this datset\n",
        "}\n",
        "\n",
        "num_round = 500  # the number of training iterations\n",
        "bst = xgb.train(param, dtrain, num_round)\n",
        "preds = bst.predict(dtest)\n",
        "import numpy as np\n",
        "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test, best_preds , normalize=True)*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ],
      "metadata": {
        "id": "HLpJAUUC7SFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Input, Embedding, Dropout\n",
        "from keras.layers import GlobalMaxPool1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.layers import CuDNNLSTM, LSTM, Bidirectional\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, Dropout, Dense, Flatten, LSTM, MaxPooling1D, Bidirectional,Conv2D\n",
        "from keras.callbacks import EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, Dropout, Dense, Flatten, LSTM, MaxPooling1D, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, TensorBoard"
      ],
      "metadata": {
        "id": "2lGVkZjz7KE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/62475807/cnn-on-tfidf-as-input\n",
        "# create model\n",
        "inp = Input(shape=(1,300))\n",
        "conv2 = Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(inp)\n",
        "drop21 = Dropout(0.2)(conv2)\n",
        "\n",
        "conv22 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(drop21)\n",
        "drop22 = Dropout(0.2)(conv22)\n",
        "conv23 = Conv1D(filters=32, kernel_size=5, activation='relu', padding='same')(drop22)\n",
        "drop23 = Dropout(0.2)(conv23)\n",
        "conv24 = Conv1D(filters=16, kernel_size=5, activation='relu', padding='same')(drop23)\n",
        "drop24 = Dropout(0.2)(conv24)\n",
        "\n",
        "pool2 = Flatten()(drop24) # this is an option to pass from 3d to 2d\n",
        "out = Dense(2, activation='softmax')(pool2) # the output dim must be equal to the num of class if u use softmax\n",
        "\n",
        "model = Model(inp, out)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(np.expand_dims(np.array(x_train), axis=1), np.array(y_train).flatten(), epochs=200)"
      ],
      "metadata": {
        "id": "nd1rG6lk7K77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1affe2f2-bb99-40e5-9d74-8bd8d512eca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1, 300)]          0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 1, 128)            192128    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1, 128)            0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 1, 64)             41024     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 1, 64)             0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 1, 32)             10272     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1, 32)             0         \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 1, 16)             2576      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1, 16)             0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 246,034\n",
            "Trainable params: 246,034\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "26/26 [==============================] - 1s 6ms/step - loss: 0.6917 - accuracy: 0.5037\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.6592 - accuracy: 0.5938\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.5175 - accuracy: 0.7901\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.4029 - accuracy: 0.8432\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3696 - accuracy: 0.8679\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3350 - accuracy: 0.8728\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3165 - accuracy: 0.8889\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.2891 - accuracy: 0.9074\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.2844 - accuracy: 0.8963\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.2542 - accuracy: 0.9062\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.2557 - accuracy: 0.9123\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.2505 - accuracy: 0.9148\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.2256 - accuracy: 0.9198\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.2434 - accuracy: 0.9160\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.2200 - accuracy: 0.9272\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.2003 - accuracy: 0.9420\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.2172 - accuracy: 0.9272\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1997 - accuracy: 0.9383\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.1987 - accuracy: 0.9358\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1860 - accuracy: 0.9370\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.1689 - accuracy: 0.9481\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1738 - accuracy: 0.9432\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1714 - accuracy: 0.9444\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.1705 - accuracy: 0.9481\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1676 - accuracy: 0.9444\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1634 - accuracy: 0.9543\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.1393 - accuracy: 0.9593\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1239 - accuracy: 0.9642\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1249 - accuracy: 0.9728\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1246 - accuracy: 0.9630\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1134 - accuracy: 0.9691\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1147 - accuracy: 0.9679\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.1193 - accuracy: 0.9691\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1102 - accuracy: 0.9580\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1101 - accuracy: 0.9617\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1254 - accuracy: 0.9605\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1065 - accuracy: 0.9667\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0848 - accuracy: 0.9753\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0873 - accuracy: 0.9753\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.1165 - accuracy: 0.9617\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0930 - accuracy: 0.9667\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0911 - accuracy: 0.9654\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.1123 - accuracy: 0.9642\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0982 - accuracy: 0.9667\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0736 - accuracy: 0.9802\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0927 - accuracy: 0.9728\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0762 - accuracy: 0.9753\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0977 - accuracy: 0.9642\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0751 - accuracy: 0.9778\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0682 - accuracy: 0.9802\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0657 - accuracy: 0.9753\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0460 - accuracy: 0.9877\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0514 - accuracy: 0.9864\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0448 - accuracy: 0.9877\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0723 - accuracy: 0.9753\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0709 - accuracy: 0.9790\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0502 - accuracy: 0.9877\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0511 - accuracy: 0.9790\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0599 - accuracy: 0.9765\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0491 - accuracy: 0.9790\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0428 - accuracy: 0.9864\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0358 - accuracy: 0.9914\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0297 - accuracy: 0.9914\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0300 - accuracy: 0.9938\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0404 - accuracy: 0.9877\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0403 - accuracy: 0.9889\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0431 - accuracy: 0.9840\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0761 - accuracy: 0.9667\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0487 - accuracy: 0.9852\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0275 - accuracy: 0.9889\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0252 - accuracy: 0.9914\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0404 - accuracy: 0.9877\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0313 - accuracy: 0.9889\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0328 - accuracy: 0.9877\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0282 - accuracy: 0.9914\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0632 - accuracy: 0.9765\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0699 - accuracy: 0.9765\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0437 - accuracy: 0.9840\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0511 - accuracy: 0.9790\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0574 - accuracy: 0.9827\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0299 - accuracy: 0.9889\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0228 - accuracy: 0.9901\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0473 - accuracy: 0.9827\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0421 - accuracy: 0.9827\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0174 - accuracy: 0.9938\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0178 - accuracy: 0.9975\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0175 - accuracy: 0.9951\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0157 - accuracy: 0.9951\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0517 - accuracy: 0.9802\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0769 - accuracy: 0.9753\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0376 - accuracy: 0.9852\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0245 - accuracy: 0.9914\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0174 - accuracy: 0.9938\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0218 - accuracy: 0.9938\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0232 - accuracy: 0.9963\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.9975\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0141 - accuracy: 0.9938\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0079 - accuracy: 0.9975\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0045 - accuracy: 0.9988\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0106 - accuracy: 0.9988\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 0.9988\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0126 - accuracy: 0.9963\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 0.9975\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9975\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0054 - accuracy: 0.9988\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0231 - accuracy: 0.9951\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 0.9975\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0047 - accuracy: 0.9988\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0076 - accuracy: 0.9975\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0159 - accuracy: 0.9951\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.1175 - accuracy: 0.9728\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.1445 - accuracy: 0.9568\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0315 - accuracy: 0.9889\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0237 - accuracy: 0.9938\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 0.9988\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0133 - accuracy: 0.9975\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0119 - accuracy: 0.9975\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0081 - accuracy: 0.9988\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9975\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0125 - accuracy: 0.9926\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0090 - accuracy: 0.9963\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 0.9988\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0170 - accuracy: 0.9963\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0201 - accuracy: 0.9914\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0205 - accuracy: 0.9938\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0359 - accuracy: 0.9864\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0140 - accuracy: 0.9951\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0254 - accuracy: 0.9926\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0100 - accuracy: 0.9975\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0100 - accuracy: 0.9975\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0076 - accuracy: 0.9963\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0095 - accuracy: 0.9963\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0091 - accuracy: 0.9975\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 0.9963\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0153 - accuracy: 0.9963\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9988\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0091 - accuracy: 0.9963\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9988\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.9963\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0050 - accuracy: 0.9988\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 0.9988\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0129 - accuracy: 0.9975\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0099 - accuracy: 0.9951\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0259 - accuracy: 0.9938\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0276 - accuracy: 0.9901\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0227 - accuracy: 0.9914\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.9963\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0154 - accuracy: 0.9926\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0189 - accuracy: 0.9963\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0073 - accuracy: 0.9988\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0050 - accuracy: 0.9988\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0072 - accuracy: 0.9975\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0037 - accuracy: 0.9988\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0255 - accuracy: 0.9951\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0306 - accuracy: 0.9926\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0389 - accuracy: 0.9889\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0430 - accuracy: 0.9938\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0243 - accuracy: 0.9926\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0364 - accuracy: 0.9901\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0200 - accuracy: 0.9901\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0122 - accuracy: 0.9975\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0243 - accuracy: 0.9951\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0136 - accuracy: 0.9975\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0310 - accuracy: 0.9951\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0055 - accuracy: 0.9988\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0041 - accuracy: 0.9988\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0049 - accuracy: 0.9975\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0219 - accuracy: 0.9938\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0142 - accuracy: 0.9963\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0089 - accuracy: 0.9988\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0068 - accuracy: 0.9988\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.9975\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0028 - accuracy: 0.9988\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0035 - accuracy: 0.9988\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 0s 16ms/step - loss: 0.0060 - accuracy: 0.9975\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 0s 14ms/step - loss: 0.0036 - accuracy: 0.9975\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 0s 12ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0073 - accuracy: 0.9975\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0218 - accuracy: 0.9926\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0310 - accuracy: 0.9938\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb8376de590>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(np.expand_dims(np.array(x_test), axis=1), np.array(y_test).flatten(), batch_size=32, verbose=1)"
      ],
      "metadata": {
        "id": "myJ2wVb07VWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9123a38e-9144-486b-e611-aeaa1dbc22c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 8ms/step - loss: 2.5024 - accuracy: 0.8111\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.50238299369812, 0.8111110925674438]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LogisticRegression"
      ],
      "metadata": {
        "id": "HIa-bRVB7_cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(random_state=0).fit(x_train, y_train)\n",
        "y_pred = clf.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "cDZGyntD8AQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd16b903-97f2-4931-b4e4-5436dde62db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8333333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLPClassifier"
      ],
      "metadata": {
        "id": "Mhounpfs8GRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(7, 2), random_state=1)\n",
        "clf.fit(x_train, y_train)\n",
        "y_pred = clf.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "PgWEVa8e8HZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "024fcbcb-4cf5-498d-a0b7-ea97315cd389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8333333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DecisionTree"
      ],
      "metadata": {
        "id": "E1MZtoBo8I5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(x_train, y_train)\n",
        "y_pred = clf.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "n74KJZ1p8Mfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bede052-df0f-42dc-a37d-dc010d099e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7111111111111111"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN"
      ],
      "metadata": {
        "id": "GT2_TkQu7s7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski')\n",
        "classifier.fit(x_train, y_train)\n",
        "y_pred = classifier.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "ltXx13um7u7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22327c99-798f-4e10-f3f9-9b2c104f52c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neighbors/_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM"
      ],
      "metadata": {
        "id": "6fbj-Mx47vdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC()\n",
        "clf.fit(x_train, y_train)\n",
        "y_pred = clf.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "huDiqSIj7xIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8afb4509-fb18-49ba-c526-28e04d1207db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8444444444444444"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kmeans"
      ],
      "metadata": {
        "id": "eR7Q0S237y-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=2, random_state=0).fit(x_train)\n",
        "y_pred = kmeans.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "c6WeEhbX715x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106d9efc-9c98-4a46-aec9-5939408c9752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.43333333333333335"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN"
      ],
      "metadata": {
        "id": "o-mj92wt72gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "clustering = DBSCAN(eps=3, min_samples=2).fit(x_train)\n",
        "y_pred = DBSCAN(eps=3, min_samples=2).fit_predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "vvxkHs5e76-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5592aa87-7bb9-4655-f664-9fb3b335b70e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.45555555555555555"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}